#!/bin/bash

# This daemon is very slow because of quvi and network overhead. It isn't so bad unless there are a lot of new videos to process.
# Initial setup (25 vids * number of subscriptions) and new subscriptions (+25 vids) are the two times where it gets really slow.
# Considering this is meant to be run with a cronjob at midnight, I find the wait acceptable.

# For future reference, assuming UPLOADER=example, the process works like this: we move data/example to data/example_old, we
# download the raw URLs into example_raw, compare the _raw URLs against _old to find new items, then we use quvi to find the
# title and append it, along with the date, URL, and uploader, to the data/example file. We then concatenate data/example and
# data/example_old into data/example_concatenated, and use that to replace the data/example file. Then, we clean up the _raw and
# _old files and repeat as necessary. After that, we do a simple numeric sort of all lines in the data dir and write the result to
# today's feed file, which is then symlinked to feed/feed.

[[ -z $XDG_CONFIG_HOME ]] && XDG_CONFIG_HOME="$HOME/.config"

# Keep non-data files out of the data dir or you will probably run into weird problems.
DATA_DIR="${XDG_CONFIG_HOME}/ytsm/data"
SUBS_FILE="${XDG_CONFIG_HOME}/ytsm/subscriptions"
FEED_DIR="${XDG_CONFIG_HOME}/ytsm/feed"

# Warn the user and/or take corrective action in the event of missing files.
[[ ! -d ${DATA_DIR} ]] && mkdir -p ${DATA_DIR} && echo "Making new data dir." >&2
[[ ! -d ${FEED_DIR} ]] && mkdir -p ${FEED_DIR} && echo "Making new feed dir." >&2
[[ ! -a ${SUBS_FILE} ]] && echo "ERROR: make a new subscription file and rerun." >&2

# This date format means we can use regular math and inequalities with no special processing.
DATE=$(date +%Y%m%d)

if [[ $1 = "--help" ]] || [[ $1 = "-h" ]]; then
	echo "To add a new subscription, add it as a new line to your subscriptions file and"
	echo "run ytsm-cronjob. Your subscriptions file should be present at"
	echo "\$XDG_CONFIG_HOME/ytsm/subscriptions, or \$HOME/.config/ytsm if \$XDG_CONFIG_HOME"
	echo "is an empty variable. To have ytsm-cronjob run automatically, type \"crontab -e\""
	echo "at a terminal and add \"@daily /path/to/ytsm-cronjob\"."
	echo ""
	echo "\$XDG_CONFIG_HOME is currently set to \"$XDG_CONFIG_HOME\""
	echo ""
	echo "Valid arguments:"
	echo " --help (-h): display this help."
	exit
fi

for i in $(seq 1 $(wc -l ${SUBS_FILE} | cut -d ' ' -f 1)); do
	UPLOADER=$(sed -n ${i}p ${SUBS_FILE})

	# Touch files to prevent errors.
	[ ! -f ${DATA_DIR}/${UPLOADER} ] && (echo "${UPLOADER} seems to be new." && touch "${DATA_DIR}/${UPLOADER}")

	# Relocate old files if need be.
	[ -f ${DATA_DIR}/${UPLOADER} ] && mv ${DATA_DIR}/${UPLOADER} ${DATA_DIR}/${UPLOADER}_old
	echo -n "${UPLOADER}: " && umph -t u $UPLOADER > ${DATA_DIR}/${UPLOADER}_raw

	# Status report:
	echo -n "${UPLOADER}: "

	# Parsing individual videos from $UPLOADER.
	for j in $(seq 1 $(wc -l ${DATA_DIR}/${UPLOADER}_raw | cut -d ' ' -f 1)); do

		# If vid is not in old file, then process the new URLs. We'll concatenate the new and old entries later.
		if [[ -z $(grep $(sed -n ${j}p ${DATA_DIR}/${UPLOADER}_raw) ${DATA_DIR}/${UPLOADER}_old) ]]; then
			URL=$(sed -n ${j}p ${DATA_DIR}/${UPLOADER}_raw)

			# QUVI_HOME has been replaced by QUVI_CONFIG in quvi 0.9. We need to hide stderr for the sake
			# of ytsm-cronjob's output regardless, so /dev/null is an easy target.
			NAME=$(QUVI_CONFIG="/dev/null" quvi dump "${URL}" 2>/dev/null | grep QUVI_MEDIA_PROPERTY_TITLE | cut -d '=' -f 2)
			
			echo -e "${DATE}\t${URL}\t${NAME}\t${UPLOADER}" >> ${DATA_DIR}/${UPLOADER}
		fi

		# Prints a dot for each video processed.
		echo -n "."
	done

	# Add "done" after the line of dots to match umph's output.
	echo " done."

	# cat and replace the regular file. This will complain if there are no new items in ${UPLOADER}, and thus no file
	# present. Because of that, we silence stderr.
	cat ${DATA_DIR}/${UPLOADER} ${DATA_DIR}/${UPLOADER}_old > ${DATA_DIR}/${UPLOADER}_concatenated 2>/dev/null
	mv ${DATA_DIR}/${UPLOADER}_concatenated ${DATA_DIR}/${UPLOADER}
	rm ${DATA_DIR}/${UPLOADER}_old ${DATA_DIR}/${UPLOADER}_raw
done

# This sorts big numbers first, which means newer items come first. A malformed data file will probably screw this up.
sort -n -r ${DATA_DIR}/* > ${FEED_DIR}/feed-${DATE} || echo "Sort failed. Check for malformed data files in ${DATA_DIR}."
[[ -a ${FEED_DIR}/feed ]] && unlink ${FEED_DIR}/feed
ln -s ${FEED_DIR}/feed-${DATE} ${FEED_DIR}/feed
